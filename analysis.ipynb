{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and imports\n",
    "\n",
    "# the usual suspects\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "# used for generating graphs\n",
    "from graphviz import Digraph\n",
    "\n",
    "# data mining algorithms\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# config\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# note: this if randomly generated fake data, not real enrolment data\n",
    "df = pd.read_csv('sample.csv')\n",
    "df.drop('UNIQUEID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Co-occurence Analysis\n",
    "\n",
    "Learning Analytics, Visual Analytics @ UBC\n",
    "\n",
    "Craig Thompson, CTLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Academic programs often prescribe some number of official pathways. However, students may also choose to take combinations of courses other than those we intend.\n",
    "\n",
    "We'd like to reveal those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![desire path](https://live.staticflickr.com/3203/2847766967_8e7ae25768_h.jpg)\n",
    "\n",
    "\n",
    "[Desire path](https://flic.kr/p/5kDxUt) by [wetwebwork](https://www.flickr.com/photos/wetwebwork/][/url]), on Flickr [(CC BY 2.0)](https://creativecommons.org/licenses/by/2.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data we have\n",
    "\n",
    "- For every student who earns a degree, we have a record of all the courses they took and counted towards their degree requirements.\n",
    "\n",
    "- We've limited the dataset to courses from a single department.\n",
    "\n",
    "- Our dataset is two dimensional **binary indicator matrix**:\n",
    "  - Across the horizontal axis: all the courses offered by the department\n",
    "  - Down the vertical axis: IDs for each student who earned a degree\n",
    "  - For each student/course pair we indicate whether the student took the course\n",
    "  - This is fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data we don't have\n",
    "\n",
    "- We don't have data for non-majors or anyone who did not complete their degree.\n",
    "\n",
    "- We don't have performance data, so we don't know how well any student did in any particular course.\n",
    "\n",
    "- We don't have any temporal information, so we don't know:\n",
    "  - Which courses students took in sequence\n",
    "  - Whether they took a pair of courses in back to back terms or with gaps\n",
    "  - Which courses they took in concurrently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the analysis\n",
    "\n",
    "- We are doing an exploratory analysis. This is not experimental.\n",
    "\n",
    "- We will try to answer questions about *what* has happened, but we are unable to address *why*.\n",
    "\n",
    "- We'd like to say \"students are discovering their own pathways through our planned degrees\". The reality is that students may be taking these groupings of courses because they:\n",
    "  - Fit nicely in their timetable\n",
    "  - Are offered by instructors they like\n",
    "  - Have a reputation of being easy or fun courses\n",
    "  - Have free or inexpensive textbooks\n",
    "  - Are the only courses left at registration time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis via clustering\n",
    "\n",
    "Common questions:\n",
    "- What does an \"average\" student look like, in terms of the courses they study?\n",
    "- If there were $N$ prototypical students, what would they look like?\n",
    "\n",
    "Answer:\n",
    "- We can formulate a *mathematically* average student, but there is no *pedagogically meaningful* average student.\n",
    "- This sort of analysis is messy and hard to interpret.\n",
    "- We'll do it anyway just to see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imshow(df.head(100), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common courses\n",
    "df.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many courses does each student take?\n",
    "df.sum(axis=1).value_counts().sort_index().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for clustering students in course-space\n",
    "\n",
    "def cluster(n, df):\n",
    "    kmedoids = KMedoids(n_clusters=n, metric='manhattan', random_state=0).fit(df)\n",
    "    nearest_medoid = kmedoids.predict(df)\n",
    "    distances = kmedoids.transform(df)\n",
    "    nearest_distance = distances[[np.arange(distances.shape[0])],nearest_medoid].T\n",
    "    return (kmedoids, nearest_medoid, distances, nearest_distance)\n",
    "\n",
    "def describe_clusters(kmedoids, nearest_medoid, distances, nearest_distance):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i in range(kmedoids.cluster_centers_.shape[0]):\n",
    "        print(\"cluster\", i+1, \"centroid:\", list(df.columns[kmedoids.cluster_centers_[i,:] == 1]))\n",
    "        print(\"number of students in this cluster:\", (nearest_medoid == i).sum())\n",
    "        cluster_member_distances = nearest_distance[nearest_medoid == i]\n",
    "        if cluster_member_distances.size > 0:\n",
    "            print(\"minimum distance to centroid:\", cluster_member_distances.min())\n",
    "            print(\"maximum distance to centroid:\", cluster_member_distances.max())\n",
    "            print(\"mean distance to centroid:\", cluster_member_distances.mean())\n",
    "            print(\"median distance to centroid:\", np.median(cluster_member_distances))\n",
    "            print()\n",
    "        plt.plot(sorted(cluster_member_distances))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_clusters(*cluster(4, df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons (re-)learned\n",
    "\n",
    "- Course enrolment datasets are big, and hard to construct a clear mental picture of.\n",
    "- We're working with (only) 50 students and 22 courses.\n",
    "- Within academic programs, there aren't usually clear, strong, non-prescribed patterns at the level of whole-enrolment histories\n",
    "\n",
    "So, let's try something different..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History\n",
    "\n",
    "What other domains work with similarly shaped data? Consumer purchases!\n",
    "\n",
    "- Each individual shopper collects a bunch of items\n",
    "- When a customer checks out, a sales invoice is generated listing all the items that were purchased together\n",
    "\n",
    "From all the sales invoices, we may wish to look for patterns in consumer behaviour:\n",
    "- Are there items that are **frequently** purchased together?\n",
    "- Are some items good **predictors** of other items being purchased?\n",
    "\n",
    "Why would someone care?\n",
    "- If consumers buy hot dogs whenever they buy hotdog buns, then grocery stores can attempt to manipulate custormers into buying hotdogs by putting hotdog buns on sale. **Profit!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content warning\n",
    "\n",
    "The following slides contain math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set theory\n",
    "\n",
    "- a *set* is an unordered collection of distinct objects.\n",
    "\n",
    "- For this analysis, each student's course enrolment history is being treated as a set. Sets are often written like this: $\\{a,b,c\\}$\n",
    "\n",
    "- All the student enrolment histories are jointly represented as a collection of sets.\n",
    "  - They are not a set-of-sets, because sets have distinct elements, and two students are able to have exactly the same course enrolment history.\n",
    "  - So, this collection of sets is called a *multiset* or a *bag*, to denote that it may contain duplicate elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent itemsets\n",
    "\n",
    "Given a multiset (such as a stack of grocery store receipts, or a table of student-course enrolments), how do we find the frequently occurring subsets (or itemsets)?\n",
    "\n",
    "Example: Given $[\\{a\\},\\{a,b\\},\\{a,c\\},\\{a,b,c,d\\}]$\n",
    "\n",
    "We can see that:\n",
    "- $\\{a\\}$ occurs in all 4 sets\n",
    "- $\\{a,b\\}$ and $\\{a,c\\}$ each occur in 2 sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<math>\n",
    "\\begin{align}\n",
    "&\\mathrm{Apriori}(T,\\epsilon)\\\\\n",
    "&\\quad L_1 \\gets \\{\\textrm{large 1 item sets}\\}\\\\\n",
    "&\\quad k \\gets 2\\\\\n",
    "&\\quad \\textbf{while}\\ L_{k-1} \\neq \\emptyset\\\\\n",
    "&\\quad \\quad C_k \\gets \\{c = a \\cup \\{b\\} \\mid a \\in L_{k-1} \\land b \\notin a, \\{s \\subseteq c \\mid |s| = k - 1 \\} \\subseteq L_{k-1} \\}\\\\\n",
    "&\\quad \\quad \\textbf{for}\\ \\textrm{transactions}\\ t \\in T\\\\\n",
    "&\\quad \\quad \\quad D_t \\gets \\{c \\in C_k \\mid c \\subseteq t \\}\\\\\n",
    "&\\quad \\quad \\quad \\textbf{for}\\ \\textrm{candidates}\\ c \\in D_t\\\\\n",
    "&\\quad \\quad \\quad \\quad count[c] \\gets count[c] + 1\\\\\n",
    "&\\quad \\quad L_k \\gets \\{c \\in C_k \\mid count[c] \\geq \\epsilon \\}\\\\\n",
    "&\\quad \\quad k \\gets k + 1\\\\\n",
    "&\\quad \\textbf{return} \\bigcup_k L_k\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "Rakesh Agrawal and Ramakrishnan Srikant. 1994. Fast Algorithms for Mining Association Rules in Large Databases. In Proceedings of the 20th International Conference on Very Large Data Bases (VLDB ’94). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 487–499. ( > 26k citations!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be an itemset and $T$ be the set of transactions/records in the database\n",
    "\n",
    "\n",
    "<math>\n",
    "\\begin{align}\n",
    "&\\textrm{Support}(X) = \\frac{\\mid \\{t \\in T \\mid X \\subseteq t \\}\\mid }{\\mid T \\mid}\n",
    "\\end{align}\n",
    "</math>\n",
    "\n",
    "\n",
    "*Support* indicates how frequently a given itemset appears in the transactions of the database.\n",
    "- A support of 1 indicates the itemset appears in every transaction.\n",
    "- A support of 0.5 indicates the itemset appears in half of the transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_frequency = apriori(df, min_support=np.nextafter(0,1), max_len=1, use_colnames=True)\n",
    "course_frequency['course'] = course_frequency['itemsets'].apply(lambda x: set(x).pop())\n",
    "course_frequency['course_number'] = course_frequency['course'].apply(lambda x: x[4:])\n",
    "course_frequency[['support', 'course']].sort_values(by='support',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = course_frequency[['support', 'course']].set_index('course').sort_values(by='support',ascending=False)\n",
    "\n",
    "def f(limit):\n",
    "    cf.head(limit).plot(kind='bar')\n",
    "\n",
    "i = interact(f, limit=(1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df, min_support=np.nextafter(0,1), max_len=2, use_colnames=True)\n",
    "frequent_itemsets.sort_values(by='support',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association rules\n",
    "\n",
    "\\begin{equation*}\n",
    "X \\Rightarrow Y, \\textrm{where}\\ X,Y \\subseteq I\n",
    "\\end{equation*}\n",
    "\n",
    "X is called the *antecedent* and Y is the *consequent*.\n",
    "\n",
    "$I$ is the set of all items (e.g. courses).\n",
    "\n",
    "example: $\\textrm{Math110} \\Rightarrow \\textrm{Math210}$ would be read as \"if Math110, then Math210\".\n",
    "\n",
    "Now we have a notation for a relationship between two itemsets (in this case, the two itemsets each contain a single item), but we need to describe the *qualities* of that relationship...\n",
    "\n",
    "Rakesh Agrawal, Tomasz Imieliński, and Arun Swami. 1993. Mining association rules between sets of items in large databases. In Proceedings of the 1993 ACM SIGMOD international conference on Management of data (SIGMOD ’93). Association for Computing Machinery, New York, NY, USA, 207–216. DOI:https://doi.org/10.1145/170035.170072 (> 22k citations!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for quantifying association rules: Support\n",
    "\n",
    "- *Antecedent Support*: indicates how frequently the antecedent item set appears in the database.\n",
    "\n",
    "$$\\textrm{Antecedent Support}(X \\Rightarrow Y) = \\frac{\\mid \\{t \\in T \\mid X \\subseteq t \\}\\mid }{\\mid T \\mid}$$\n",
    "\n",
    "- *Consequent Support*: indicates how frequently the consequent item set appears in the database.\n",
    "\n",
    "$$\\textrm{Consequent Support}(X \\Rightarrow Y) = \\frac{\\mid \\{t \\in T \\mid Y \\subseteq t \\}\\mid }{\\mid T \\mid}$$\n",
    "\n",
    "- *(Rule) Support*: indicates how frequently the all them items of the antecedent and consequent jointly appear in the database.\n",
    "\n",
    "$$\\textrm{Support}(X \\Rightarrow Y) = \\frac{\\mid \\{t \\in T \\mid X \\cup Y \\subseteq t \\}\\mid }{\\mid T \\mid}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for quantifying association rules: Confidence\n",
    "\n",
    "$$\\textrm{Confidence}(X \\Rightarrow Y) = \\frac{ \\textrm{Support}(X \\Rightarrow Y) }{ \\textrm{Support}(X) }$$\n",
    "\n",
    "*Confidence*: the ratio of rule support to antecedent support. \n",
    "  - Or, given that the antecedent has been observed, how likely are we to also observe the consequent?\n",
    "\n",
    "If a rule has high confidence, and the antecedent is observed, then we can be fairly confident that the consequent will be observed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for quantifying association rules: Lift\n",
    "\n",
    "$$\\textrm{Lift}(X \\Rightarrow Y) = \\frac{ \\textrm{Confidence}(X \\Rightarrow Y) }{ \\textrm{Support}(Y) }$$\n",
    "\n",
    "*Lift*: ratio of confidence to consequent support. Lift is a measure of how much more often the antecedent and the consequent occur together than would be expected if they were statistically independent. When the antecedent of a rule with high lift is observed, we can be more confident that the consequent will also be observed.\n",
    "\n",
    "Confidence and lift are both descriptors of the \"power\" of a rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=np.nextafter(0,1))\n",
    "\n",
    "rules['antecedent_course'] = rules['antecedents'].apply(lambda x: set(x).pop())\n",
    "rules['consequent_course'] = rules['consequents'].apply(lambda x: set(x).pop())\n",
    "\n",
    "rules['antecedent_course_number'] = rules['antecedent_course'].apply(lambda x: int(x[4:]))\n",
    "rules['consequent_course_number'] = rules['consequent_course'].apply(lambda x: int(x[4:]))\n",
    "\n",
    "rules['antecedent_year_level'] = rules['antecedent_course_number'].apply(lambda x: x//100 )\n",
    "rules['consequent_year_level'] = rules['consequent_course_number'].apply(lambda x: x//100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_rules = rules[(rules['antecedent_year_level']==3) & (rules['consequent_year_level']==3)]\n",
    "pairwise_support = pairwise_rules.pivot(index='antecedent_course',columns='consequent_course',values='support').fillna(0)\n",
    "ax = sns.heatmap(pairwise_support, xticklabels=True, yticklabels=True, cmap='BuPu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_rules = rules[(rules['antecedent_year_level']==3) & (rules['consequent_year_level']==3)]\n",
    "pairwise_confidence = pairwise_rules.pivot(index='antecedent_course',columns='consequent_course',values='confidence').fillna(0)\n",
    "ax = sns.heatmap(pairwise_confidence, xticklabels=True, yticklabels=True, cmap='BuPu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_rules = rules[(rules['antecedent_year_level']==3) & (rules['consequent_year_level']==3)]\n",
    "pairwise_lift = pairwise_rules.pivot(index='antecedent_course',columns='consequent_course',values='lift').fillna(0.1)\n",
    "#pairwise_lift = pairwise_lift.applymap(lambda x: x if x >=1 else 0.01)\n",
    "ax = sns.heatmap(pairwise_lift, xticklabels=True, yticklabels=True, cmap='BuPu', norm=LogNorm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring 'significant' rules\n",
    "sig_rules = rules[\n",
    "      (rules['support'] > 0.01)\n",
    "    #& (rules['antecedent support'] > 0.01)\n",
    "    #& (rules['consequent support'] > 0.01)\n",
    "    & (rules['antecedent_year_level'] <= rules['consequent_year_level'])\n",
    "    & (rules['confidence'] > 0.5)\n",
    "    & (rules['lift'] > 1.5)\n",
    "                 ].sort_values(by='lift',ascending=False)\n",
    "sig_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rules(sig_rules):\n",
    "    antecedents = sig_rules[['antecedent_course','antecedent_course_number']]\n",
    "    antecedents.columns = ['course','course_number']\n",
    "\n",
    "    consequents = sig_rules[['consequent_course','consequent_course_number']]\n",
    "    consequents.columns = ['course','course_number']\n",
    "\n",
    "    figure_courses = pd.concat([antecedents, consequents]).drop_duplicates()\n",
    "\n",
    "    dot = Digraph()\n",
    "    for course in figure_courses.itertuples():\n",
    "        dot.node(str(course.course_number),course.course)\n",
    "\n",
    "    for association in sig_rules.itertuples():\n",
    "        dot.edge(str(association.antecedent_course_number), str(association.consequent_course_number), label=f\"{association.lift:.2f}\")\n",
    "\n",
    "    dot.graph_attr['overlap'] = 'False'\n",
    "    dot.engine = 'neato'\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = plot_rules(sig_rules)\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What next?\n",
    "\n",
    "- This analysis was (nearly) totally naive to the official structure of the courses/programs offered. That means we 'discovered' several results that are obvious and uninteresting to anyone with domain expertise.\n",
    "  - For example, we may have 'discovered' that if you take a senior course, then you are very likely to also take the prerequisites.\n",
    "  - We should filter the result set to remove any association rules that reflect formal prerequisite relationships.\n",
    "\n",
    "- We've looked at *what* courses students take, but we have not investigated ordering, or the effects of ordering. An analysis of sequencing would be useful for planning connections across courses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You\n",
    "\n",
    "Questions?\n",
    "\n",
    "Contact: craig.thompson@ubc.ca"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
